{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 67553,
          "databundleVersionId": 7504710,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30635,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyojit02/efficinet_net_based_cnn_model/blob/main/notebook0d1ab38f9f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-21T12:19:14.557433Z",
          "iopub.execute_input": "2024-01-21T12:19:14.558274Z",
          "iopub.status.idle": "2024-01-21T12:19:16.161134Z",
          "shell.execute_reply.started": "2024-01-21T12:19:14.558223Z",
          "shell.execute_reply": "2024-01-21T12:19:16.160132Z"
        },
        "trusted": true,
        "id": "e1-VDvJ0bE-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU availability\n",
        "if tf.test.is_gpu_available():\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    print(\"No GPU detected. Please install a compatible GPU and make sure the drivers are properly configured.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T06:03:28.336848Z",
          "iopub.execute_input": "2024-01-21T06:03:28.337320Z",
          "iopub.status.idle": "2024-01-21T06:03:41.175228Z",
          "shell.execute_reply.started": "2024-01-21T06:03:28.337290Z",
          "shell.execute_reply": "2024-01-21T06:03:41.174245Z"
        },
        "trusted": true,
        "id": "LHUHUq4gbE-b",
        "outputId": "ba0f6d60-8eb0-4e23-91ad-7e6da76285fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GPU is available.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "# from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import cv2\n",
        "\n",
        "# Use absolute paths for better clarity\n",
        "abs_train_path = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/train.csv'\n",
        "abs_train_directory = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/train'\n",
        "abs_test_path = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/sample_submission.csv'\n",
        "abs_test_directory = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/test'\n",
        "\n",
        "df = pd.read_csv(abs_train_path)\n",
        "df2 = pd.read_csv(abs_test_path)\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "# Resize and convert to grayscale for the training set\n",
        "for i in range(df.shape[0]):\n",
        "    img = cv2.imread(os.path.join(abs_train_directory, df.iloc[i]['file_name']), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the image using cv2\n",
        "    img_resized = cv2.resize(img, (180, 180))\n",
        "\n",
        "    # Expand dimensions to (180, 180, 1)\n",
        "    img_resized = np.expand_dims(img_resized, axis=-1)\n",
        "\n",
        "    X_train.append(img_resized)\n",
        "    y_train.append(df.iloc[i]['label'])\n",
        "    if i % 100 == 0:\n",
        "        print(i)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "# Resize and convert to grayscale for the test set\n",
        "for i in range(df2.shape[0]):\n",
        "    img = cv2.imread(os.path.join(abs_test_directory, df2.iloc[i]['file_name']), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the image using cv2\n",
        "    img_resized = cv2.resize(img, (180, 180))\n",
        "\n",
        "    # Expand dimensions to (180, 180, 1)\n",
        "    img_resized = np.expand_dims(img_resized, axis=-1)\n",
        "\n",
        "    X_test.append(img_resized)\n",
        "    y_test.append(df2.iloc[i]['label'])\n",
        "    if i % 100 == 0:\n",
        "        print(i)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train_np, X_test_np = np.array(X_train) / 255.0, np.array(X_test) / 255.0\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T06:51:28.032264Z",
          "iopub.execute_input": "2024-01-21T06:51:28.032662Z",
          "iopub.status.idle": "2024-01-21T06:54:26.134895Z",
          "shell.execute_reply.started": "2024-01-21T06:51:28.032629Z",
          "shell.execute_reply": "2024-01-21T06:54:26.133987Z"
        },
        "trusted": true,
        "id": "jaeWyoX9bE-d",
        "outputId": "f3aa5df0-0b5e-4fb8-fa7a-0a80e9b755c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\n18100\n18200\n18300\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T06:57:59.451831Z",
          "iopub.execute_input": "2024-01-21T06:57:59.452867Z",
          "iopub.status.idle": "2024-01-21T06:57:59.460560Z",
          "shell.execute_reply.started": "2024-01-21T06:57:59.452822Z",
          "shell.execute_reply": "2024-01-21T06:57:59.459464Z"
        },
        "trusted": true,
        "id": "YfBZxbsPbE-d",
        "outputId": "cdfb9471-c61b-4e63-d9de-fdb07bde07ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, callbacks\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Data augmentation without ResNet50 preprocessing\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "print(\"done 1\\n\")\n",
        "\n",
        "# Create custom CNN model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add convolutional layers\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flatten the output and add dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "print(\"done 2\\n\")\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(set(y_train))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(\"done 3\\n\")\n",
        "\n",
        "# Learning rate scheduling\n",
        "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Model checkpointing\n",
        "checkpoint_filepath = 'best_model.h5'\n",
        "model_checkpoint = callbacks.ModelCheckpoint(\n",
        "    checkpoint_filepath,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "print(\"done 4\\n\")\n",
        "\n",
        "# Train the model with data augmentation\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train_np, np.array(y_train), batch_size=32),\n",
        "    epochs=30,\n",
        "    validation_data=(X_test_np, np.array(y_test)),\n",
        "    callbacks=[lr_scheduler, model_checkpoint]\n",
        ")\n",
        "print(\"done 5\\n\")\n",
        "\n",
        "# Load the best model\n",
        "model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test_np)\n",
        "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T06:58:07.893387Z",
          "iopub.execute_input": "2024-01-21T06:58:07.894390Z",
          "iopub.status.idle": "2024-01-21T07:21:01.760967Z",
          "shell.execute_reply.started": "2024-01-21T06:58:07.894352Z",
          "shell.execute_reply": "2024-01-21T07:21:01.759863Z"
        },
        "trusted": true,
        "id": "ZnPuyXSxbE-e",
        "outputId": "ca77496d-de65-447f-c608-6dc56692cd54"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "done 1\n\ndone 2\n\ndone 3\n\ndone 4\n\nEpoch 1/30\n575/575 [==============================] - ETA: 0s - loss: 2.6858 - accuracy: 0.1842\nEpoch 1: val_loss improved from inf to 3.31419, saving model to best_model.h5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "575/575 [==============================] - 53s 82ms/step - loss: 2.6858 - accuracy: 0.1842 - val_loss: 3.3142 - val_accuracy: 0.0040 - lr: 1.0000e-04\nEpoch 2/30\n575/575 [==============================] - ETA: 0s - loss: 2.3934 - accuracy: 0.2128\nEpoch 2: val_loss did not improve from 3.31419\n575/575 [==============================] - 44s 77ms/step - loss: 2.3934 - accuracy: 0.2128 - val_loss: 3.8689 - val_accuracy: 1.7388e-04 - lr: 1.0000e-04\nEpoch 3/30\n575/575 [==============================] - ETA: 0s - loss: 2.2372 - accuracy: 0.2329\nEpoch 3: val_loss did not improve from 3.31419\n575/575 [==============================] - 45s 77ms/step - loss: 2.2372 - accuracy: 0.2329 - val_loss: 3.3763 - val_accuracy: 0.0000e+00 - lr: 1.0000e-04\nEpoch 4/30\n575/575 [==============================] - ETA: 0s - loss: 2.1040 - accuracy: 0.2557\nEpoch 4: val_loss improved from 3.31419 to 3.28064, saving model to best_model.h5\n575/575 [==============================] - 46s 79ms/step - loss: 2.1040 - accuracy: 0.2557 - val_loss: 3.2806 - val_accuracy: 0.0000e+00 - lr: 1.0000e-04\nEpoch 5/30\n575/575 [==============================] - ETA: 0s - loss: 2.0053 - accuracy: 0.2699\nEpoch 5: val_loss improved from 3.28064 to 2.82899, saving model to best_model.h5\n575/575 [==============================] - 45s 78ms/step - loss: 2.0053 - accuracy: 0.2699 - val_loss: 2.8290 - val_accuracy: 0.0012 - lr: 1.0000e-04\nEpoch 6/30\n575/575 [==============================] - ETA: 0s - loss: 1.9517 - accuracy: 0.2769\nEpoch 6: val_loss improved from 2.82899 to 2.55123, saving model to best_model.h5\n575/575 [==============================] - 45s 78ms/step - loss: 1.9517 - accuracy: 0.2769 - val_loss: 2.5512 - val_accuracy: 0.0110 - lr: 1.0000e-04\nEpoch 7/30\n575/575 [==============================] - ETA: 0s - loss: 1.8896 - accuracy: 0.2888\nEpoch 7: val_loss did not improve from 2.55123\n575/575 [==============================] - 45s 78ms/step - loss: 1.8896 - accuracy: 0.2888 - val_loss: 2.9288 - val_accuracy: 0.0035 - lr: 1.0000e-04\nEpoch 8/30\n575/575 [==============================] - ETA: 0s - loss: 1.8317 - accuracy: 0.3013\nEpoch 8: val_loss did not improve from 2.55123\n575/575 [==============================] - 45s 78ms/step - loss: 1.8317 - accuracy: 0.3013 - val_loss: 2.7669 - val_accuracy: 0.0014 - lr: 1.0000e-04\nEpoch 9/30\n575/575 [==============================] - ETA: 0s - loss: 1.7920 - accuracy: 0.3124\nEpoch 9: val_loss did not improve from 2.55123\n575/575 [==============================] - 44s 77ms/step - loss: 1.7920 - accuracy: 0.3124 - val_loss: 3.2964 - val_accuracy: 3.4777e-04 - lr: 1.0000e-04\nEpoch 10/30\n575/575 [==============================] - ETA: 0s - loss: 1.7456 - accuracy: 0.3255\nEpoch 10: val_loss did not improve from 2.55123\n575/575 [==============================] - 44s 77ms/step - loss: 1.7456 - accuracy: 0.3255 - val_loss: 2.7491 - val_accuracy: 3.4777e-04 - lr: 1.0000e-05\nEpoch 11/30\n575/575 [==============================] - ETA: 0s - loss: 1.7435 - accuracy: 0.3252\nEpoch 11: val_loss did not improve from 2.55123\n575/575 [==============================] - 44s 77ms/step - loss: 1.7435 - accuracy: 0.3252 - val_loss: 2.8164 - val_accuracy: 3.4777e-04 - lr: 1.0000e-05\nEpoch 12/30\n575/575 [==============================] - ETA: 0s - loss: 1.7263 - accuracy: 0.3348\nEpoch 12: val_loss did not improve from 2.55123\n575/575 [==============================] - 44s 76ms/step - loss: 1.7263 - accuracy: 0.3348 - val_loss: 2.7293 - val_accuracy: 0.0012 - lr: 1.0000e-05\nEpoch 13/30\n575/575 [==============================] - ETA: 0s - loss: 1.7141 - accuracy: 0.3332\nEpoch 13: val_loss did not improve from 2.55123\n575/575 [==============================] - 45s 78ms/step - loss: 1.7141 - accuracy: 0.3332 - val_loss: 2.7566 - val_accuracy: 8.6941e-04 - lr: 1.0000e-06\nEpoch 14/30\n575/575 [==============================] - ETA: 0s - loss: 1.7189 - accuracy: 0.3364\nEpoch 14: val_loss did not improve from 2.55123\n575/575 [==============================] - 44s 77ms/step - loss: 1.7189 - accuracy: 0.3364 - val_loss: 2.7785 - val_accuracy: 0.0010 - lr: 1.0000e-06\nEpoch 15/30\n575/575 [==============================] - ETA: 0s - loss: 1.7109 - accuracy: 0.3371\nEpoch 15: val_loss did not improve from 2.55123\n575/575 [==============================] - 45s 78ms/step - loss: 1.7109 - accuracy: 0.3371 - val_loss: 2.7780 - val_accuracy: 0.0010 - lr: 1.0000e-06\nEpoch 16/30\n575/575 [==============================] - ETA: 0s - loss: 1.7136 - accuracy: 0.3413\nEpoch 16: val_loss did not improve from 2.55123\n575/575 [==============================] - 45s 78ms/step - loss: 1.7136 - accuracy: 0.3413 - val_loss: 2.7932 - val_accuracy: 0.0010 - lr: 1.0000e-06\nEpoch 17/30\n575/575 [==============================] - ETA: 0s - loss: 1.7104 - accuracy: 0.3396\nEpoch 17: val_loss did not improve from 2.55123\n575/575 [==============================] - 45s 77ms/step - loss: 1.7104 - accuracy: 0.3396 - val_loss: 2.7529 - val_accuracy: 0.0010 - lr: 1.0000e-06\nEpoch 18/30\n575/575 [==============================] - ETA: 0s - loss: 1.7113 - accuracy: 0.3360\nEpoch 18: val_loss did not improve from 2.55123\n575/575 [==============================] - 44s 77ms/step - loss: 1.7113 - accuracy: 0.3360 - val_loss: 2.7583 - val_accuracy: 0.0012 - lr: 1.0000e-06\nEpoch 19/30\n575/575 [==============================] - ETA: 0s - loss: 1.7104 - accuracy: 0.3390\nEpoch 19: val_loss did not improve from 2.55123\n575/575 [==============================] - 45s 78ms/step - loss: 1.7104 - accuracy: 0.3390 - val_loss: 2.7866 - val_accuracy: 6.9553e-04 - lr: 1.0000e-06\nEpoch 20/30\n575/575 [==============================] - ETA: 0s - loss: 1.7163 - accuracy: 0.3381\nEpoch 20: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 80ms/step - loss: 1.7163 - accuracy: 0.3381 - val_loss: 2.8006 - val_accuracy: 0.0014 - lr: 1.0000e-06\nEpoch 21/30\n575/575 [==============================] - ETA: 0s - loss: 1.7067 - accuracy: 0.3410\nEpoch 21: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 81ms/step - loss: 1.7067 - accuracy: 0.3410 - val_loss: 2.7788 - val_accuracy: 0.0012 - lr: 1.0000e-06\nEpoch 22/30\n575/575 [==============================] - ETA: 0s - loss: 1.7081 - accuracy: 0.3396\nEpoch 22: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 81ms/step - loss: 1.7081 - accuracy: 0.3396 - val_loss: 2.7571 - val_accuracy: 0.0017 - lr: 1.0000e-06\nEpoch 23/30\n575/575 [==============================] - ETA: 0s - loss: 1.7008 - accuracy: 0.3417\nEpoch 23: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 80ms/step - loss: 1.7008 - accuracy: 0.3417 - val_loss: 2.7601 - val_accuracy: 0.0014 - lr: 1.0000e-06\nEpoch 24/30\n575/575 [==============================] - ETA: 0s - loss: 1.7141 - accuracy: 0.3369\nEpoch 24: val_loss did not improve from 2.55123\n575/575 [==============================] - 47s 81ms/step - loss: 1.7141 - accuracy: 0.3369 - val_loss: 2.8091 - val_accuracy: 0.0014 - lr: 1.0000e-06\nEpoch 25/30\n575/575 [==============================] - ETA: 0s - loss: 1.7076 - accuracy: 0.3414\nEpoch 25: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 81ms/step - loss: 1.7076 - accuracy: 0.3414 - val_loss: 2.7948 - val_accuracy: 0.0014 - lr: 1.0000e-06\nEpoch 26/30\n575/575 [==============================] - ETA: 0s - loss: 1.7074 - accuracy: 0.3393\nEpoch 26: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 80ms/step - loss: 1.7074 - accuracy: 0.3393 - val_loss: 2.7711 - val_accuracy: 0.0019 - lr: 1.0000e-06\nEpoch 27/30\n575/575 [==============================] - ETA: 0s - loss: 1.7073 - accuracy: 0.3394\nEpoch 27: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 80ms/step - loss: 1.7073 - accuracy: 0.3394 - val_loss: 2.7718 - val_accuracy: 0.0014 - lr: 1.0000e-06\nEpoch 28/30\n575/575 [==============================] - ETA: 0s - loss: 1.6967 - accuracy: 0.3410\nEpoch 28: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 81ms/step - loss: 1.6967 - accuracy: 0.3410 - val_loss: 2.7848 - val_accuracy: 0.0012 - lr: 1.0000e-06\nEpoch 29/30\n575/575 [==============================] - ETA: 0s - loss: 1.7075 - accuracy: 0.3416\nEpoch 29: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 80ms/step - loss: 1.7075 - accuracy: 0.3416 - val_loss: 2.7732 - val_accuracy: 0.0016 - lr: 1.0000e-06\nEpoch 30/30\n575/575 [==============================] - ETA: 0s - loss: 1.6981 - accuracy: 0.3435\nEpoch 30: val_loss did not improve from 2.55123\n575/575 [==============================] - 46s 81ms/step - loss: 1.6981 - accuracy: 0.3435 - val_loss: 2.7630 - val_accuracy: 0.0019 - lr: 1.0000e-06\ndone 5\n\n180/180 [==============================] - 2s 8ms/step\nAccuracy: 0.010954616588419406\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with the file_name and label columns\n",
        "df_output = pd.DataFrame({'file_name': df2[\"file_name\"], 'label': predicted_labels})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_output.to_csv('output.csv', index=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T07:40:33.685759Z",
          "iopub.execute_input": "2024-01-21T07:40:33.686657Z",
          "iopub.status.idle": "2024-01-21T07:40:33.710559Z",
          "shell.execute_reply.started": "2024-01-21T07:40:33.686623Z",
          "shell.execute_reply": "2024-01-21T07:40:33.709609Z"
        },
        "trusted": true,
        "id": "TChYkw9UbE-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T07:40:44.045991Z",
          "iopub.execute_input": "2024-01-21T07:40:44.046379Z",
          "iopub.status.idle": "2024-01-21T07:40:44.063115Z",
          "shell.execute_reply.started": "2024-01-21T07:40:44.046350Z",
          "shell.execute_reply": "2024-01-21T07:40:44.062095Z"
        },
        "trusted": true,
        "id": "wwXUC7xobE-e",
        "outputId": "67665223-c77f-474c-a7a9-febcba560539"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                 file_name  label\n0     Image_test_00001.jpg      3\n1     Image_test_00002.jpg      3\n2     Image_test_00003.jpg      2\n3     Image_test_00004.jpg      2\n4     Image_test_00005.jpg      3\n...                    ...    ...\n5746  Image_test_05747.jpg      2\n5747  Image_test_05748.jpg      2\n5748  Image_test_05749.jpg      3\n5749  Image_test_05750.jpg      2\n5750  Image_test_05751.jpg      3\n\n[5751 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Image_test_00001.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Image_test_00002.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Image_test_00003.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Image_test_00004.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Image_test_00005.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5746</th>\n      <td>Image_test_05747.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5747</th>\n      <td>Image_test_05748.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5748</th>\n      <td>Image_test_05749.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5749</th>\n      <td>Image_test_05750.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5750</th>\n      <td>Image_test_05751.jpg</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5751 rows Ã— 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    }
  ]
}